# ETL Pipeline in Databricks

## ðŸ“Œ Overview
This project implements a simple ETL (Extract, Transform, Load) pipeline in **Databricks** using **PySpark**. The pipeline follows the **Medallion Architecture**:
- **Bronze Layer**: Raw data ingestion
- **Silver Layer**: Data cleaning and transformation
- **Gold Layer**: Aggregated and ready-for-use data

## ðŸ“‚ Project Structure
```
â”‚â”€â”€ etl/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ bronze_ingestion.py
â”‚   â”œâ”€â”€ transformation/
â”‚   â”‚   â”œâ”€â”€ silver_transformation.py
â”‚   â”œâ”€â”€ aggregation/
â”‚   â”‚   â”œâ”€â”€ gold_aggregation.py
â”‚â”€â”€ scripts/
â”‚   â”œâ”€â”€ run_etl.py   <-- Runs the entire ETL pipeline
â”‚â”€â”€ config/
â”‚   â”œâ”€â”€ settings.py  <-- Configuration settings
â”‚â”€â”€ tests/
â”‚   â”œâ”€â”€ test_silver.py
â”‚â”€â”€ README.md
```

## ðŸš€ How to Run
### 1ï¸âƒ£ Run Manually Using `run_etl.py`
If running locally or in Databricks Notebooks:
```bash
python3 scripts/run_etl.py
```
This will:
1. Load data into the **Bronze** layer
2. Clean and transform it into the **Silver** layer
3. Aggregate and store results in the **Gold** layer

### 2ï¸âƒ£ Run Using Databricks Workflows (Recommended)
1. Create **3 Workflows** in Databricks:
   - **Job 1** â†’ Runs `bronze_ingestion.py`
   - **Job 2** â†’ Runs `silver_transformation.py`
   - **Job 3** â†’ Runs `gold_aggregation.py`
2. **Set Dependencies** so that:
   - Job 2 runs **after** Job 1
   - Job 3 runs **after** Job 2

## ðŸ” Unit Tests
Unit tests are included using `pytest`. Run them via:
```bash
pytest tests/
```

## ðŸ› ï¸ Configuration
Edit `config/settings.py` to update paths, storage locations, or other settings.

## ðŸ“Œ Notes
- If using **Databricks Repos**, ensure the workspace is properly synced.
- If using **GitHub Actions**, ensure the CI/CD pipeline deploys correctly.